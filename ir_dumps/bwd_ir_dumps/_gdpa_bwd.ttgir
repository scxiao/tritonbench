#blocked = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#linear = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 16], [0, 32]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[32, 0], [64, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 16], [0, 32], [32, 0]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 8], [0, 16], [0, 32], [32, 0]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 4]], warp = [[0, 0], [0, 0]], block = []}>
#loc = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1520:0)
#mma = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 1], instrShape = [32, 32], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 4, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared2 = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [0, 1]}>
#smem = #ttg.shared_memory
#loc192 = loc("Q"(#loc))
#loc193 = loc("Q_offsets"(#loc))
#loc194 = loc("K"(#loc))
#loc195 = loc("K_offsets"(#loc))
#loc196 = loc("V"(#loc))
#loc197 = loc("DO"(#loc))
#loc198 = loc("Out_offsets"(#loc))
#loc199 = loc("DQ"(#loc))
#loc200 = loc("DK"(#loc))
#loc201 = loc("DV"(#loc))
#loc202 = loc("stride_qm"(#loc))
#loc203 = loc("stride_km"(#loc))
#loc204 = loc("stride_qh"(#loc))
#loc205 = loc("stride_kh"(#loc))
#loc206 = loc("stride_dom"(#loc))
#loc207 = loc("stride_doh"(#loc))
#loc208 = loc("Z"(#loc))
#loc209 = loc("H"(#loc))
#loc210 = loc("N_CTX"(#loc))
#loc211 = loc("N_CTX_KV"(#loc))
#loc212 = loc("qk_scale"(#loc))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_gdpa_bwd(%Q: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Q"(#loc)), %Q_offsets: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Q_offsets"(#loc)), %K: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("K"(#loc)), %K_offsets: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("K_offsets"(#loc)), %V: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("V"(#loc)), %DO: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DO"(#loc)), %Out_offsets: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Out_offsets"(#loc)), %DQ: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DQ"(#loc)), %DK: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DK"(#loc)), %DV: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DV"(#loc)), %stride_qm: i32 {tt.divisibility = 16 : i32} loc("stride_qm"(#loc)), %stride_km: i32 {tt.divisibility = 16 : i32} loc("stride_km"(#loc)), %stride_qh: i32 {tt.divisibility = 16 : i32} loc("stride_qh"(#loc)), %stride_kh: i32 {tt.divisibility = 16 : i32} loc("stride_kh"(#loc)), %stride_dom: i32 {tt.divisibility = 16 : i32} loc("stride_dom"(#loc)), %stride_doh: i32 {tt.divisibility = 16 : i32} loc("stride_doh"(#loc)), %Z: i32 {tt.divisibility = 16 : i32} loc("Z"(#loc)), %H: i32 loc("H"(#loc)), %N_CTX: i32 {tt.divisibility = 16 : i32} loc("N_CTX"(#loc)), %N_CTX_KV: i32 {tt.divisibility = 16 : i32} loc("N_CTX_KV"(#loc)), %qk_scale: f32 loc("qk_scale"(#loc))) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_0 = arith.constant dense<2.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_1 = arith.constant dense<0.107032225> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_3 = arith.constant dense<1.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_4 = arith.constant dense<4.471500e-02> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_5 = arith.constant dense<0.797884583> : tensor<128x64xf32, #mma> loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc213)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c64_i32 = arith.constant 64 : i32 loc(#loc213)
    %c63_i32 = arith.constant 63 : i32 loc(#loc213)
    %true = arith.constant true loc(#loc1)
    %cst_6 = arith.constant dense<64> : tensor<64x1xi32, #blocked> loc(#loc213)
    %cst_7 = arith.constant dense<64> : tensor<1x64xi32, #blocked1> loc(#loc1)
    %cst_8 = arith.constant dense<64> : tensor<1x64xi32, #linear> loc(#loc213)
    llvm.intr.assume %true : i1 loc(#loc3)
    llvm.intr.assume %true : i1 loc(#loc4)
    llvm.intr.assume %true : i1 loc(#loc5)
    llvm.intr.assume %true : i1 loc(#loc6)
    llvm.intr.assume %true : i1 loc(#loc7)
    llvm.intr.assume %true : i1 loc(#loc8)
    llvm.intr.assume %true : i1 loc(#loc9)
    %off_z = tt.get_program_id z : i32 loc(#loc214)
    %off_seq_h = tt.get_program_id x : i32 loc(#loc215)
    %off_h = arith.remsi %off_seq_h, %H : i32 loc(#loc216)
    %pid = arith.divsi %off_seq_h, %H : i32 loc(#loc217)
    %begin_q = tt.addptr %Q_offsets, %off_z : !tt.ptr<i32>, i32 loc(#loc373)
    %begin_q_9 = tt.load %begin_q : !tt.ptr<i32> loc(#loc374)
    %end_q = tt.addptr %begin_q, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc375)
    %end_q_10 = tt.load %end_q : !tt.ptr<i32> loc(#loc376)
    %qlen = arith.subi %end_q_10, %begin_q_9 : i32 loc(#loc377)
    %begin_k = tt.addptr %K_offsets, %off_z : !tt.ptr<i32>, i32 loc(#loc378)
    %begin_k_11 = tt.load %begin_k : !tt.ptr<i32> loc(#loc379)
    %end_k = tt.addptr %begin_k, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc380)
    %end_k_12 = tt.load %end_k : !tt.ptr<i32> loc(#loc381)
    %klen = arith.subi %end_k_12, %begin_k_11 : i32 loc(#loc382)
    %start_n = arith.muli %pid, %c128_i32 : i32 loc(#loc383)
    %offs_k = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc384)
    %offs_k_13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc384)
    %0 = arith.cmpi sle, %start_n, %klen : i32 loc(#loc230)
    %1 = arith.cmpi sle, %start_n, %qlen : i32 loc(#loc231)
    %2 = arith.ori %0, %1 : i1 loc(#loc232)
    %3:7 = scf.if %2 -> (!tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>) {
      %begin_o = tt.addptr %Out_offsets, %off_z : !tt.ptr<i32>, i32 loc(#loc385)
      %begin_o_14 = tt.load %begin_o : !tt.ptr<i32> loc(#loc386)
      %off_h2 = arith.extsi %off_h : i32 to i64 loc(#loc387)
      %qadj = arith.extsi %stride_qh : i32 to i64 loc(#loc388)
      %qadj_15 = arith.muli %off_h2, %qadj : i64 loc(#loc388)
      %qadj_16 = arith.muli %begin_q_9, %stride_qm : i32 loc(#loc389)
      %qadj_17 = arith.extsi %qadj_16 : i32 to i64 loc(#loc390)
      %qadj_18 = arith.addi %qadj_15, %qadj_17 : i64 loc(#loc390)
      %kadj = arith.muli %off_h, %stride_kh : i32 loc(#loc391)
      %kadj_19 = arith.muli %begin_k_11, %stride_km : i32 loc(#loc392)
      %kadj_20 = arith.addi %kadj, %kadj_19 : i32 loc(#loc393)
      %doadj = arith.extsi %stride_doh : i32 to i64 loc(#loc394)
      %doadj_21 = arith.muli %off_h2, %doadj : i64 loc(#loc394)
      %doadj_22 = arith.muli %begin_o_14, %stride_dom : i32 loc(#loc395)
      %doadj_23 = arith.extsi %doadj_22 : i32 to i64 loc(#loc396)
      %doadj_24 = arith.addi %doadj_21, %doadj_23 : i64 loc(#loc396)
      %Q_25 = tt.addptr %Q, %qadj_18 : !tt.ptr<bf16>, i64 loc(#loc488)
      %K_26 = tt.addptr %K, %kadj_20 : !tt.ptr<bf16>, i32 loc(#loc489)
      %V_27 = tt.addptr %V, %kadj_20 : !tt.ptr<bf16>, i32 loc(#loc490)
      %DO_28 = tt.addptr %DO, %doadj_24 : !tt.ptr<bf16>, i64 loc(#loc491)
      %DQ_29 = tt.addptr %DQ, %qadj_18 : !tt.ptr<bf16>, i64 loc(#loc492)
      %DK_30 = tt.addptr %DK, %kadj_20 : !tt.ptr<bf16>, i32 loc(#loc493)
      %DV_31 = tt.addptr %DV, %kadj_20 : !tt.ptr<bf16>, i32 loc(#loc494)
      scf.yield %Q_25, %K_26, %V_27, %DO_28, %DQ_29, %DK_30, %DV_31 : !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16> loc(#loc494)
    } else {
      scf.yield %Q, %K, %V, %DO, %DQ, %DK, %DV : !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16>, !tt.ptr<bf16> loc(#loc213)
    } loc(#loc233)
    %4 = arith.cmpi slt, %start_n, %klen : i32 loc(#loc253)
    scf.if %4 {
      %offs_n = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc404)
      %offs_n_14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc404)
      %offs_n_15 = tt.splat %start_n : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc405)
      %offs_n_16 = tt.splat %start_n : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc405)
      %offs_n_17 = arith.addi %offs_n_15, %offs_n : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc405)
      %offs_n_18 = arith.addi %offs_n_16, %offs_n_14 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc405)
      %kmask = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc406)
      %kmask_19 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc406)
      %kmask_20 = tt.expand_dims %kmask {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi32, #linear> loc(#loc406)
      %kmask_21 = tt.expand_dims %kmask_19 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc406)
      %kmask_22 = arith.cmpi slt, %kmask_20, %cst_8 : tensor<1x64xi32, #linear> loc(#loc407)
      %kmask_23 = arith.cmpi slt, %kmask_21, %cst_7 : tensor<1x64xi32, #blocked1> loc(#loc407)
      %kmask_24 = tt.expand_dims %offs_n_17 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc408)
      %kmask_25 = tt.expand_dims %offs_n_18 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc408)
      %kmask_26 = tt.splat %klen : i32 -> tensor<128x1xi32, #linear> loc(#loc409)
      %kmask_27 = tt.splat %klen : i32 -> tensor<128x1xi32, #blocked1> loc(#loc409)
      %kmask_28 = arith.cmpi slt, %kmask_24, %kmask_26 : tensor<128x1xi32, #linear> loc(#loc409)
      %kmask_29 = arith.cmpi slt, %kmask_25, %kmask_27 : tensor<128x1xi32, #blocked1> loc(#loc409)
      %kmask_30 = tt.broadcast %kmask_22 : tensor<1x64xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc410)
      %kmask_31 = tt.broadcast %kmask_23 : tensor<1x64xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc410)
      %kmask_32 = tt.broadcast %kmask_28 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc410)
      %kmask_33 = tt.broadcast %kmask_29 : tensor<128x1xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc410)
      %kmask_34 = arith.andi %kmask_30, %kmask_32 : tensor<128x64xi1, #linear> loc(#loc410)
      %kmask_35 = arith.andi %kmask_31, %kmask_33 : tensor<128x64xi1, #blocked1> loc(#loc410)
      %k = tt.expand_dims %offs_n_14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc411)
      %k_36 = arith.muli %start_n, %stride_km : i32 loc(#loc411)
      %k_37 = tt.splat %stride_km : i32 -> tensor<128x1xi32, #blocked1> loc(#loc411)
      %k_38 = arith.muli %k, %k_37 : tensor<128x1xi32, #blocked1> loc(#loc411)
      %k_39 = tt.addptr %3#1, %k_36 : !tt.ptr<bf16>, i32 loc(#loc411)
      %k_40 = tt.broadcast %k_38 : tensor<128x1xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc412)
      %k_41 = tt.broadcast %kmask_21 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc412)
      %k_42 = arith.addi %k_41, %k_40 : tensor<128x64xi32, #blocked1> loc(#loc412)
      %k_43 = amdgpu.buffer_load %k_39[%k_42], %kmask_35 stride = %stride_km : tensor<128x64xbf16, #blocked1> loc(#loc413)
      %k_44 = ttg.local_alloc %k_43 : (tensor<128x64xbf16, #blocked1>) -> !ttg.memdesc<128x64xbf16, #shared, #smem> loc(#loc413)
      %k_45 = ttg.local_load %k_44 : !ttg.memdesc<128x64xbf16, #shared, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc413)
      %v = tt.addptr %3#2, %k_36 : !tt.ptr<bf16>, i32 loc(#loc414)
      %v_46 = amdgpu.buffer_load %v[%k_42], %kmask_35 stride = %stride_km : tensor<128x64xbf16, #blocked1> loc(#loc415)
      %v_47 = ttg.local_alloc %v_46 : (tensor<128x64xbf16, #blocked1>) -> !ttg.memdesc<128x64xbf16, #shared1, #smem> loc(#loc415)
      %v_48 = ttg.local_load %v_47 : !ttg.memdesc<128x64xbf16, #shared1, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc415)
      %num_steps = arith.addi %qlen, %c63_i32 : i32 loc(#loc495)
      %num_steps_49 = arith.divsi %num_steps, %c64_i32 : i32 loc(#loc496)
      %qT_ptrs = tt.expand_dims %offs_k {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc417)
      %qT_ptrs_50 = tt.splat %stride_qm : i32 -> tensor<1x64xi32, #blocked> loc(#loc417)
      %qT_ptrs_51 = arith.muli %qT_ptrs, %qT_ptrs_50 : tensor<1x64xi32, #blocked> loc(#loc417)
      %qT_ptrs_52 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc418)
      %qT_ptrs_53 = tt.expand_dims %qT_ptrs_52 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc418)
      %qT_ptrs_54 = tt.broadcast %qT_ptrs_51 : tensor<1x64xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc419)
      %qT_ptrs_55 = tt.broadcast %qT_ptrs_53 : tensor<64x1xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc419)
      %qT_ptrs_56 = arith.addi %qT_ptrs_55, %qT_ptrs_54 : tensor<64x64xi32, #blocked> loc(#loc419)
      %do_ptrs = tt.expand_dims %offs_k_13 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi32, #blocked1> loc(#loc420)
      %do_ptrs_57 = tt.splat %stride_dom : i32 -> tensor<64x1xi32, #blocked1> loc(#loc420)
      %do_ptrs_58 = arith.muli %do_ptrs, %do_ptrs_57 : tensor<64x1xi32, #blocked1> loc(#loc420)
      %do_ptrs_59 = tt.broadcast %do_ptrs_58 : tensor<64x1xi32, #blocked1> -> tensor<64x64xi32, #blocked1> loc(#loc421)
      %do_ptrs_60 = tt.broadcast %kmask_21 : tensor<1x64xi32, #blocked1> -> tensor<64x64xi32, #blocked1> loc(#loc421)
      %do_ptrs_61 = arith.addi %do_ptrs_60, %do_ptrs_59 : tensor<64x64xi32, #blocked1> loc(#loc421)
      %qmask = arith.cmpi slt, %qT_ptrs_53, %cst_6 : tensor<64x1xi32, #blocked> loc(#loc422)
      %qmask_62 = tt.splat %qlen : i32 -> tensor<1x64xi32, #blocked> loc(#loc423)
      %qmask_63 = tt.broadcast %qmask : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc424)
      %omask = tt.splat %qlen : i32 -> tensor<64x1xi32, #blocked1> loc(#loc425)
      %omask_64 = tt.broadcast %kmask_23 : tensor<1x64xi1, #blocked1> -> tensor<64x64xi1, #blocked1> loc(#loc426)
      %ppT = tt.splat %qk_scale : f32 -> tensor<128x64xf32, #mma> loc(#loc427)
      %qT_ptrs_65 = arith.muli %stride_qm, %c64_i32 : i32 loc(#loc428)
      %do_ptrs_66 = arith.muli %stride_dom, %c64_i32 : i32 loc(#loc429)
      %curr_m = arith.cmpi sgt, %num_steps_49, %c0_i32 : i32 loc(#loc560)
      %qmask_67 = arith.cmpi slt, %qT_ptrs, %qmask_62 : tensor<1x64xi32, #blocked> loc(#loc423)
      %qmask_68 = tt.broadcast %qmask_67 : tensor<1x64xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc424)
      %qmask_69 = arith.andi %qmask_63, %qmask_68 : tensor<64x64xi1, #blocked> loc(#loc424)
      %curr_m_70 = tt.splat %curr_m : i1 -> tensor<64x64xi1, #blocked> loc(#loc560)
      %curr_m_71 = arith.andi %curr_m_70, %qmask_69 : tensor<64x64xi1, #blocked> loc(#loc560)
      %qT = amdgpu.buffer_load %3#0[%qT_ptrs_56], %curr_m_71 stride = %stride_qm : tensor<64x64xbf16, #blocked> loc(#loc431)
      %omask_72 = arith.cmpi slt, %do_ptrs, %omask : tensor<64x1xi32, #blocked1> loc(#loc425)
      %omask_73 = tt.broadcast %omask_72 : tensor<64x1xi1, #blocked1> -> tensor<64x64xi1, #blocked1> loc(#loc426)
      %omask_74 = arith.andi %omask_73, %omask_64 : tensor<64x64xi1, #blocked1> loc(#loc426)
      %curr_m_75 = tt.splat %curr_m : i1 -> tensor<64x64xi1, #blocked1> loc(#loc560)
      %curr_m_76 = arith.andi %curr_m_75, %omask_74 : tensor<64x64xi1, #blocked1> loc(#loc560)
      %do = amdgpu.buffer_load %3#3[%do_ptrs_61], %curr_m_76 stride = %stride_dom : tensor<64x64xbf16, #blocked1> loc(#loc432)
      %curr_m_77 = arith.subi %num_steps_49, %c1_i32 : i32 loc(#loc560)
      %curr_m_78:7 = scf.for %curr_m_124 = %c0_i32 to %curr_m_77 step %c1_i32 iter_args(%arg22 = %cst, %arg23 = %cst, %arg24 = %c0_i32, %qT_ptrs_125 = %3#0, %do_ptrs_126 = %3#3, %qT_127 = %qT, %do_128 = %do) -> (tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, !tt.ptr<bf16>, !tt.ptr<bf16>, tensor<64x64xbf16, #blocked>, tensor<64x64xbf16, #blocked1>)  : i32 {
        %curr_m_129 = arith.addi %arg24, %c64_i32 : i32 loc(#loc433)
        %qT_ptrs_130 = tt.addptr %qT_ptrs_125, %qT_ptrs_65 : !tt.ptr<bf16>, i32 loc(#loc434)
        %do_ptrs_131 = tt.addptr %do_ptrs_126, %do_ptrs_66 : !tt.ptr<bf16>, i32 loc(#loc435)
        %offs_m = tt.splat %curr_m_129 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc436)
        %offs_m_132 = tt.splat %curr_m_129 : i32 -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc436)
        %offs_m_133 = arith.addi %offs_m, %offs_k : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc436)
        %offs_m_134 = arith.addi %offs_m_132, %offs_k_13 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc436)
        %qmask_135 = tt.expand_dims %offs_m_133 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc437)
        %qmask_136 = arith.cmpi slt, %qmask_135, %qmask_62 : tensor<1x64xi32, #blocked> loc(#loc423)
        %qmask_137 = tt.broadcast %qmask_136 : tensor<1x64xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc424)
        %qmask_138 = arith.andi %qmask_63, %qmask_137 : tensor<64x64xi1, #blocked> loc(#loc424)
        %qT_139 = amdgpu.buffer_load %qT_ptrs_130[%qT_ptrs_56], %qmask_138 stride = %stride_qm : tensor<64x64xbf16, #blocked> loc(#loc431)
        %qT_140 = ttg.local_alloc %qT_127 : (tensor<64x64xbf16, #blocked>) -> !ttg.memdesc<64x64xbf16, #shared2, #smem> loc(#loc431)
        %dk_141 = ttg.local_load %qT_140 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #linear1> loc(#loc438)
        %dk_142 = tt.trans %dk_141 {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear1> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc439)
        %qT_143 = ttg.local_load %qT_140 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc431)
        %qkT_144 = tt.dot %k_45, %qT_143, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc440)
        %omask_145 = tt.expand_dims %offs_m_134 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi32, #blocked1> loc(#loc441)
        %omask_146 = arith.cmpi slt, %omask_145, %omask : tensor<64x1xi32, #blocked1> loc(#loc425)
        %omask_147 = tt.broadcast %omask_146 : tensor<64x1xi1, #blocked1> -> tensor<64x64xi1, #blocked1> loc(#loc426)
        %omask_148 = arith.andi %omask_147, %omask_64 : tensor<64x64xi1, #blocked1> loc(#loc426)
        %do_149 = amdgpu.buffer_load %do_ptrs_131[%do_ptrs_61], %omask_148 stride = %stride_dom : tensor<64x64xbf16, #blocked1> loc(#loc432)
        %do_150 = ttg.local_alloc %do_128 : (tensor<64x64xbf16, #blocked1>) -> !ttg.memdesc<64x64xbf16, #shared1, #smem> loc(#loc432)
        %dpT_151 = ttg.local_load %do_150 : !ttg.memdesc<64x64xbf16, #shared1, #smem> -> tensor<64x64xbf16, #linear2> loc(#loc442)
        %dpT_152 = tt.trans %dpT_151 {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear2> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc443)
        %do_153 = ttg.local_load %do_150 : !ttg.memdesc<64x64xbf16, #shared1, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc432)
        %dpT_154 = tt.dot %v_48, %dpT_152, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x64xf32, #mma> loc(#loc442)
        %tanh_out_155 = arith.mulf %qkT_144, %cst_5 : tensor<128x64xf32, #mma> loc(#loc444)
        %tanh_out_156 = arith.mulf %qkT_144, %cst_4 : tensor<128x64xf32, #mma> loc(#loc445)
        %tanh_out_157 = arith.mulf %tanh_out_156, %qkT_144 : tensor<128x64xf32, #mma> loc(#loc446)
        %tanh_out_158 = arith.addf %tanh_out_157, %cst_3 : tensor<128x64xf32, #mma> loc(#loc447)
        %tanh_out_159 = arith.mulf %tanh_out_155, %tanh_out_158 : tensor<128x64xf32, #mma> loc(#loc448)
        %tanh_out_160 = arith.mulf %tanh_out_159, %cst_0 : tensor<128x64xf32, #mma> loc(#loc525)
        %tanh_out_161 = arith.subf %cst, %tanh_out_160 : tensor<128x64xf32, #mma> loc(#loc548)
        %tanh_out_162 = math.exp %tanh_out_161 : tensor<128x64xf32, #mma> loc(#loc549)
        %tanh_out_163 = arith.addf %tanh_out_162, %cst_3 : tensor<128x64xf32, #mma> loc(#loc550)
        %tanh_out_164 = arith.divf %cst_3, %tanh_out_163 : tensor<128x64xf32, #mma> loc(#loc551)
        %tanh_out_165 = arith.mulf %tanh_out_164, %cst_0 : tensor<128x64xf32, #mma> loc(#loc527)
        %tanh_out_166 = arith.subf %tanh_out_165, %cst_3 : tensor<128x64xf32, #mma> loc(#loc528)
        %ppT_167 = arith.mulf %qkT_144, %cst_2 : tensor<128x64xf32, #mma> loc(#loc450)
        %ppT_168 = arith.addf %tanh_out_166, %cst_3 : tensor<128x64xf32, #mma> loc(#loc451)
        %ppT_169 = arith.mulf %ppT_167, %ppT_168 : tensor<128x64xf32, #mma> loc(#loc452)
        %ppT_170 = arith.mulf %ppT_169, %ppT : tensor<128x64xf32, #mma> loc(#loc427)
        %ppT_171 = arith.truncf %ppT_170 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc453)
        %ppT_172 = ttg.convert_layout %ppT_171 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc453)
        %dv_173 = tt.dot %ppT_172, %do_153, %arg23 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x64xf32, #mma> loc(#loc454)
        %pT_174 = arith.mulf %tanh_out_166, %tanh_out_166 : tensor<128x64xf32, #mma> loc(#loc455)
        %pT_175 = arith.subf %cst_3, %pT_174 : tensor<128x64xf32, #mma> loc(#loc456)
        %pT_176 = arith.mulf %ppT_167, %pT_175 : tensor<128x64xf32, #mma> loc(#loc457)
        %pT_177 = arith.mulf %qkT_144, %cst_1 : tensor<128x64xf32, #mma> loc(#loc458)
        %pT_178 = arith.mulf %pT_177, %qkT_144 : tensor<128x64xf32, #mma> loc(#loc459)
        %pT_179 = arith.addf %pT_178, %cst_5 : tensor<128x64xf32, #mma> loc(#loc460)
        %pT_180 = arith.mulf %pT_176, %pT_179 : tensor<128x64xf32, #mma> loc(#loc461)
        %pT_181 = arith.mulf %ppT_168, %cst_2 : tensor<128x64xf32, #mma> loc(#loc462)
        %pT_182 = arith.addf %pT_180, %pT_181 : tensor<128x64xf32, #mma> loc(#loc463)
        %pT_183 = arith.mulf %pT_182, %ppT : tensor<128x64xf32, #mma> loc(#loc464)
        %dsT_184 = arith.mulf %pT_183, %dpT_154 : tensor<128x64xf32, #mma> loc(#loc465)
        %dsT_185 = arith.truncf %dsT_184 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc466)
        %dsT_186 = ttg.convert_layout %dsT_185 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc466)
        %dk_187 = tt.dot %dsT_186, %dk_142, %arg22 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc438)
        scf.yield %dk_187, %dv_173, %curr_m_129, %qT_ptrs_130, %do_ptrs_131, %qT_139, %do_149 : tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, !tt.ptr<bf16>, !tt.ptr<bf16>, tensor<64x64xbf16, #blocked>, tensor<64x64xbf16, #blocked1> loc(#loc560)
      } {tt.loop_unroll_factor = 1 : i32} loc(#loc560)
      %curr_m_79 = arith.cmpi sge, %num_steps_49, %c1_i32 : i32 loc(#loc560)
      %qT_80 = ttg.local_alloc %curr_m_78#5 : (tensor<64x64xbf16, #blocked>) -> !ttg.memdesc<64x64xbf16, #shared2, #smem> loc(#loc431)
      %dk = ttg.local_load %qT_80 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #linear1> loc(#loc438)
      %dk_81 = tt.trans %dk {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear1> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc439)
      %qT_82 = ttg.local_load %qT_80 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc431)
      %qkT = scf.if %curr_m_79 -> (tensor<128x64xf32, #mma>) {
        %qkT_124 = tt.dot %k_45, %qT_82, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc440)
        scf.yield %qkT_124 : tensor<128x64xf32, #mma> loc(#loc440)
      } else {
        scf.yield %cst : tensor<128x64xf32, #mma> loc(#loc440)
      } loc(#loc440)
      %do_83 = ttg.local_alloc %curr_m_78#6 : (tensor<64x64xbf16, #blocked1>) -> !ttg.memdesc<64x64xbf16, #shared1, #smem> loc(#loc432)
      %dpT = ttg.local_load %do_83 : !ttg.memdesc<64x64xbf16, #shared1, #smem> -> tensor<64x64xbf16, #linear2> loc(#loc442)
      %dpT_84 = tt.trans %dpT {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear2> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc443)
      %do_85 = ttg.local_load %do_83 : !ttg.memdesc<64x64xbf16, #shared1, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc432)
      %dpT_86 = scf.if %curr_m_79 -> (tensor<128x64xf32, #mma>) {
        %dpT_124 = tt.dot %v_48, %dpT_84, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x64xf32, #mma> loc(#loc442)
        scf.yield %dpT_124 : tensor<128x64xf32, #mma> loc(#loc442)
      } else {
        scf.yield %cst : tensor<128x64xf32, #mma> loc(#loc442)
      } loc(#loc442)
      %tanh_out = arith.mulf %qkT, %cst_5 : tensor<128x64xf32, #mma> loc(#loc444)
      %tanh_out_87 = arith.mulf %qkT, %cst_4 : tensor<128x64xf32, #mma> loc(#loc445)
      %tanh_out_88 = arith.mulf %tanh_out_87, %qkT : tensor<128x64xf32, #mma> loc(#loc446)
      %tanh_out_89 = arith.addf %tanh_out_88, %cst_3 : tensor<128x64xf32, #mma> loc(#loc447)
      %tanh_out_90 = arith.mulf %tanh_out, %tanh_out_89 : tensor<128x64xf32, #mma> loc(#loc448)
      %tanh_out_91 = arith.mulf %tanh_out_90, %cst_0 : tensor<128x64xf32, #mma> loc(#loc525)
      %tanh_out_92 = arith.subf %cst, %tanh_out_91 : tensor<128x64xf32, #mma> loc(#loc548)
      %tanh_out_93 = math.exp %tanh_out_92 : tensor<128x64xf32, #mma> loc(#loc549)
      %tanh_out_94 = arith.addf %tanh_out_93, %cst_3 : tensor<128x64xf32, #mma> loc(#loc550)
      %tanh_out_95 = arith.divf %cst_3, %tanh_out_94 : tensor<128x64xf32, #mma> loc(#loc551)
      %tanh_out_96 = arith.mulf %tanh_out_95, %cst_0 : tensor<128x64xf32, #mma> loc(#loc527)
      %tanh_out_97 = arith.subf %tanh_out_96, %cst_3 : tensor<128x64xf32, #mma> loc(#loc528)
      %ppT_98 = arith.mulf %qkT, %cst_2 : tensor<128x64xf32, #mma> loc(#loc450)
      %ppT_99 = arith.addf %tanh_out_97, %cst_3 : tensor<128x64xf32, #mma> loc(#loc451)
      %ppT_100 = arith.mulf %ppT_98, %ppT_99 : tensor<128x64xf32, #mma> loc(#loc452)
      %ppT_101 = arith.mulf %ppT_100, %ppT : tensor<128x64xf32, #mma> loc(#loc427)
      %ppT_102 = arith.truncf %ppT_101 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc453)
      %ppT_103 = ttg.convert_layout %ppT_102 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc453)
      %dv = scf.if %curr_m_79 -> (tensor<128x64xf32, #mma>) {
        %dv_124 = tt.dot %ppT_103, %do_85, %curr_m_78#1 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x64xf32, #mma> loc(#loc454)
        scf.yield %dv_124 : tensor<128x64xf32, #mma> loc(#loc454)
      } else {
        scf.yield %curr_m_78#1 : tensor<128x64xf32, #mma> loc(#loc454)
      } loc(#loc454)
      %pT = arith.mulf %tanh_out_97, %tanh_out_97 : tensor<128x64xf32, #mma> loc(#loc455)
      %pT_104 = arith.subf %cst_3, %pT : tensor<128x64xf32, #mma> loc(#loc456)
      %pT_105 = arith.mulf %ppT_98, %pT_104 : tensor<128x64xf32, #mma> loc(#loc457)
      %pT_106 = arith.mulf %qkT, %cst_1 : tensor<128x64xf32, #mma> loc(#loc458)
      %pT_107 = arith.mulf %pT_106, %qkT : tensor<128x64xf32, #mma> loc(#loc459)
      %pT_108 = arith.addf %pT_107, %cst_5 : tensor<128x64xf32, #mma> loc(#loc460)
      %pT_109 = arith.mulf %pT_105, %pT_108 : tensor<128x64xf32, #mma> loc(#loc461)
      %pT_110 = arith.mulf %ppT_99, %cst_2 : tensor<128x64xf32, #mma> loc(#loc462)
      %pT_111 = arith.addf %pT_109, %pT_110 : tensor<128x64xf32, #mma> loc(#loc463)
      %pT_112 = arith.mulf %pT_111, %ppT : tensor<128x64xf32, #mma> loc(#loc464)
      %dsT = arith.mulf %pT_112, %dpT_86 : tensor<128x64xf32, #mma> loc(#loc465)
      %dsT_113 = arith.truncf %dsT : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc466)
      %dsT_114 = ttg.convert_layout %dsT_113 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc466)
      %dk_115 = scf.if %curr_m_79 -> (tensor<128x64xf32, #mma>) {
        %dk_124 = tt.dot %dsT_114, %dk_81, %curr_m_78#0 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc438)
        scf.yield %dk_124 : tensor<128x64xf32, #mma> loc(#loc438)
      } else {
        scf.yield %curr_m_78#0 : tensor<128x64xf32, #mma> loc(#loc438)
      } loc(#loc438)
      %curr_m_116 = arith.select %curr_m_79, %dk_115, %curr_m_78#0 : tensor<128x64xf32, #mma> loc(#loc560)
      %curr_m_117 = arith.select %curr_m_79, %dv, %curr_m_78#1 : tensor<128x64xf32, #mma> loc(#loc560)
      %dv_ptrs = tt.expand_dims %offs_n {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc467)
      %dv_ptrs_118 = tt.splat %stride_km : i32 -> tensor<128x1xi32, #linear> loc(#loc467)
      %dv_ptrs_119 = arith.muli %dv_ptrs, %dv_ptrs_118 : tensor<128x1xi32, #linear> loc(#loc467)
      %dv_ptrs_120 = tt.addptr %3#6, %k_36 : !tt.ptr<bf16>, i32 loc(#loc467)
      %dv_ptrs_121 = tt.broadcast %dv_ptrs_119 : tensor<128x1xi32, #linear> -> tensor<128x64xi32, #linear> loc(#loc468)
      %dv_ptrs_122 = tt.broadcast %kmask_20 : tensor<1x64xi32, #linear> -> tensor<128x64xi32, #linear> loc(#loc468)
      %dv_ptrs_123 = arith.addi %dv_ptrs_122, %dv_ptrs_121 : tensor<128x64xi32, #linear> loc(#loc468)
      %6 = arith.truncf %curr_m_117 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc321)
      %7 = ttg.convert_layout %6 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #linear> loc(#loc321)
      amdgpu.buffer_store %7, %dv_ptrs_120[%dv_ptrs_123], %kmask_34 stride = %stride_km : tensor<128x64xbf16, #linear> loc(#loc321)
      %dk_ptrs = tt.addptr %3#5, %k_36 : !tt.ptr<bf16>, i32 loc(#loc469)
      %8 = arith.truncf %curr_m_116 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc323)
      %9 = ttg.convert_layout %8 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #linear> loc(#loc323)
      amdgpu.buffer_store %9, %dk_ptrs[%dv_ptrs_123], %kmask_34 stride = %stride_km : tensor<128x64xbf16, #linear> loc(#loc323)
    } loc(#loc254)
    %5 = arith.cmpi slt, %start_n, %qlen : i32 loc(#loc324)
    scf.if %5 {
      %offs_m = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc470)
      %offs_m_14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc470)
      %offs_m_15 = tt.splat %start_n : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc471)
      %offs_m_16 = tt.splat %start_n : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc471)
      %offs_m_17 = arith.addi %offs_m_15, %offs_m : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc471)
      %offs_m_18 = arith.addi %offs_m_16, %offs_m_14 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc471)
      %qmask = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc472)
      %qmask_19 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc472)
      %qmask_20 = tt.expand_dims %qmask {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi32, #linear> loc(#loc472)
      %qmask_21 = tt.expand_dims %qmask_19 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc472)
      %qmask_22 = arith.cmpi slt, %qmask_20, %cst_8 : tensor<1x64xi32, #linear> loc(#loc473)
      %qmask_23 = arith.cmpi slt, %qmask_21, %cst_7 : tensor<1x64xi32, #blocked1> loc(#loc473)
      %qmask_24 = tt.expand_dims %offs_m_17 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc474)
      %qmask_25 = tt.expand_dims %offs_m_18 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc474)
      %qmask_26 = tt.splat %qlen : i32 -> tensor<128x1xi32, #linear> loc(#loc475)
      %qmask_27 = tt.splat %qlen : i32 -> tensor<128x1xi32, #blocked1> loc(#loc475)
      %qmask_28 = arith.cmpi slt, %qmask_24, %qmask_26 : tensor<128x1xi32, #linear> loc(#loc475)
      %qmask_29 = arith.cmpi slt, %qmask_25, %qmask_27 : tensor<128x1xi32, #blocked1> loc(#loc475)
      %qmask_30 = tt.broadcast %qmask_22 : tensor<1x64xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc476)
      %qmask_31 = tt.broadcast %qmask_23 : tensor<1x64xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc476)
      %qmask_32 = tt.broadcast %qmask_28 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc476)
      %qmask_33 = tt.broadcast %qmask_29 : tensor<128x1xi1, #blocked1> -> tensor<128x64xi1, #blocked1> loc(#loc476)
      %qmask_34 = arith.andi %qmask_30, %qmask_32 : tensor<128x64xi1, #linear> loc(#loc476)
      %qmask_35 = arith.andi %qmask_31, %qmask_33 : tensor<128x64xi1, #blocked1> loc(#loc476)
      %q = tt.expand_dims %offs_m_14 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc477)
      %q_36 = arith.muli %start_n, %stride_qm : i32 loc(#loc477)
      %q_37 = tt.splat %stride_qm : i32 -> tensor<128x1xi32, #blocked1> loc(#loc477)
      %q_38 = arith.muli %q, %q_37 : tensor<128x1xi32, #blocked1> loc(#loc477)
      %q_39 = tt.addptr %3#0, %q_36 : !tt.ptr<bf16>, i32 loc(#loc477)
      %q_40 = tt.broadcast %q_38 : tensor<128x1xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc478)
      %q_41 = tt.broadcast %qmask_21 : tensor<1x64xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc478)
      %q_42 = arith.addi %q_41, %q_40 : tensor<128x64xi32, #blocked1> loc(#loc478)
      %q_43 = amdgpu.buffer_load %q_39[%q_42], %qmask_35 stride = %stride_qm : tensor<128x64xbf16, #blocked1> loc(#loc479)
      %q_44 = ttg.local_alloc %q_43 : (tensor<128x64xbf16, #blocked1>) -> !ttg.memdesc<128x64xbf16, #shared, #smem> loc(#loc479)
      %q_45 = ttg.local_load %q_44 : !ttg.memdesc<128x64xbf16, #shared, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc479)
      %do = arith.muli %start_n, %stride_dom : i32 loc(#loc480)
      %do_46 = tt.splat %stride_dom : i32 -> tensor<128x1xi32, #blocked1> loc(#loc480)
      %do_47 = arith.muli %q, %do_46 : tensor<128x1xi32, #blocked1> loc(#loc480)
      %do_48 = tt.addptr %3#3, %do : !tt.ptr<bf16>, i32 loc(#loc480)
      %do_49 = tt.broadcast %do_47 : tensor<128x1xi32, #blocked1> -> tensor<128x64xi32, #blocked1> loc(#loc481)
      %do_50 = arith.addi %q_41, %do_49 : tensor<128x64xi32, #blocked1> loc(#loc481)
      %do_51 = amdgpu.buffer_load %do_48[%do_50], %qmask_35 stride = %stride_dom : tensor<128x64xbf16, #blocked1> loc(#loc482)
      %do_52 = ttg.local_alloc %do_51 : (tensor<128x64xbf16, #blocked1>) -> !ttg.memdesc<128x64xbf16, #shared, #smem> loc(#loc482)
      %do_53 = ttg.local_load %do_52 : !ttg.memdesc<128x64xbf16, #shared, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc482)
      %num_steps = arith.addi %klen, %c63_i32 : i32 loc(#loc499)
      %num_steps_54 = arith.divsi %num_steps, %c64_i32 : i32 loc(#loc500)
      %kT_ptrs = tt.expand_dims %offs_k {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc501)
      %kT_ptrs_55 = tt.splat %stride_km : i32 -> tensor<1x64xi32, #blocked> loc(#loc501)
      %kT_ptrs_56 = arith.muli %kT_ptrs, %kT_ptrs_55 : tensor<1x64xi32, #blocked> loc(#loc501)
      %kT_ptrs_57 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc502)
      %kT_ptrs_58 = tt.expand_dims %kT_ptrs_57 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc502)
      %kT_ptrs_59 = tt.broadcast %kT_ptrs_56 : tensor<1x64xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc503)
      %kT_ptrs_60 = tt.broadcast %kT_ptrs_58 : tensor<64x1xi32, #blocked> -> tensor<64x64xi32, #blocked> loc(#loc503)
      %kT_ptrs_61 = arith.addi %kT_ptrs_60, %kT_ptrs_59 : tensor<64x64xi32, #blocked> loc(#loc503)
      %kmask = arith.cmpi slt, %kT_ptrs_58, %cst_6 : tensor<64x1xi32, #blocked> loc(#loc504)
      %kmask_62 = tt.splat %klen : i32 -> tensor<1x64xi32, #blocked> loc(#loc505)
      %kmask_63 = tt.broadcast %kmask : tensor<64x1xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc506)
      %p = tt.splat %qk_scale : f32 -> tensor<128x64xf32, #mma> loc(#loc507)
      %kT_ptrs_64 = arith.muli %stride_km, %c64_i32 : i32 loc(#loc508)
      %vT = ttg.local_alloc : () -> !ttg.memdesc<1x64x64xbf16, #shared2, #smem, mutable> loc(#loc509)
      %curr_n = arith.cmpi sgt, %num_steps_54, %c0_i32 : i32 loc(#loc555)
      %kmask_65 = arith.cmpi slt, %kT_ptrs, %kmask_62 : tensor<1x64xi32, #blocked> loc(#loc505)
      %kmask_66 = tt.broadcast %kmask_65 : tensor<1x64xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc506)
      %kmask_67 = arith.andi %kmask_63, %kmask_66 : tensor<64x64xi1, #blocked> loc(#loc506)
      %curr_n_68 = tt.splat %curr_n : i1 -> tensor<64x64xi1, #blocked> loc(#loc555)
      %curr_n_69 = arith.andi %curr_n_68, %kmask_67 : tensor<64x64xi1, #blocked> loc(#loc555)
      %kT = amdgpu.buffer_load %3#1[%kT_ptrs_61], %curr_n_69 stride = %stride_km : tensor<64x64xbf16, #blocked> loc(#loc511)
      %vT_70 = amdgpu.buffer_load %3#2[%kT_ptrs_61], %curr_n_69 stride = %stride_km : tensor<64x64xbf16, #blocked> loc(#loc509)
      %vT_71 = ttg.memdesc_index %vT[%c0_i32] : !ttg.memdesc<1x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> loc(#loc509)
      ttg.local_store %vT_70, %vT_71 : tensor<64x64xbf16, #blocked> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> loc(#loc509)
      %curr_n_72 = arith.subi %num_steps_54, %c1_i32 : i32 loc(#loc555)
      %curr_n_73:7 = scf.for %curr_n_112 = %c0_i32 to %curr_n_72 step %c1_i32 iter_args(%arg22 = %cst, %arg23 = %c0_i32, %kT_ptrs_113 = %3#1, %vT_ptrs = %3#2, %arg26 = %c0_i32, %kT_114 = %kT, %vT_115 = %vT_71) -> (tensor<128x64xf32, #mma>, i32, !tt.ptr<bf16>, !tt.ptr<bf16>, i32, tensor<64x64xbf16, #blocked>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64>)  : i32 {
        %curr_n_116 = arith.addi %arg23, %c64_i32 : i32 loc(#loc512)
        %kT_ptrs_117 = tt.addptr %kT_ptrs_113, %kT_ptrs_64 : !tt.ptr<bf16>, i32 loc(#loc513)
        %vT_ptrs_118 = tt.addptr %vT_ptrs, %kT_ptrs_64 : !tt.ptr<bf16>, i32 loc(#loc514)
        %offs_n = tt.splat %curr_n_116 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc515)
        %offs_n_119 = arith.addi %offs_n, %offs_k : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc515)
        %kmask_120 = tt.expand_dims %offs_n_119 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc516)
        %kmask_121 = arith.cmpi slt, %kmask_120, %kmask_62 : tensor<1x64xi32, #blocked> loc(#loc505)
        %kmask_122 = tt.broadcast %kmask_121 : tensor<1x64xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc506)
        %kmask_123 = arith.andi %kmask_63, %kmask_122 : tensor<64x64xi1, #blocked> loc(#loc506)
        %kT_124 = amdgpu.buffer_load %kT_ptrs_117[%kT_ptrs_61], %kmask_123 stride = %stride_km : tensor<64x64xbf16, #blocked> loc(#loc511)
        %kT_125 = ttg.local_alloc %kT_114 : (tensor<64x64xbf16, #blocked>) -> !ttg.memdesc<64x64xbf16, #shared2, #smem> loc(#loc511)
        %dq_126 = ttg.local_load %kT_125 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #linear1> loc(#loc517)
        %dq_127 = tt.trans %dq_126 {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear1> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc518)
        %kT_128 = ttg.local_load %kT_125 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc511)
        %vT_129 = amdgpu.buffer_load %vT_ptrs_118[%kT_ptrs_61], %kmask_123 stride = %stride_km : tensor<64x64xbf16, #blocked> loc(#loc509)
        %vT_130 = ttg.local_load %vT_115 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc509)
        %qk_131 = tt.dot %q_45, %kT_128, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc519)
        %tanh_out_132 = arith.mulf %qk_131, %cst_5 : tensor<128x64xf32, #mma> loc(#loc530)
        %tanh_out_133 = arith.mulf %qk_131, %cst_4 : tensor<128x64xf32, #mma> loc(#loc531)
        %tanh_out_134 = arith.mulf %tanh_out_133, %qk_131 : tensor<128x64xf32, #mma> loc(#loc532)
        %tanh_out_135 = arith.addf %tanh_out_134, %cst_3 : tensor<128x64xf32, #mma> loc(#loc533)
        %tanh_out_136 = arith.mulf %tanh_out_132, %tanh_out_135 : tensor<128x64xf32, #mma> loc(#loc534)
        %tanh_out_137 = arith.mulf %tanh_out_136, %cst_0 : tensor<128x64xf32, #mma> loc(#loc556)
        %tanh_out_138 = arith.subf %cst, %tanh_out_137 : tensor<128x64xf32, #mma> loc(#loc561)
        %tanh_out_139 = math.exp %tanh_out_138 : tensor<128x64xf32, #mma> loc(#loc562)
        %tanh_out_140 = arith.addf %tanh_out_139, %cst_3 : tensor<128x64xf32, #mma> loc(#loc563)
        %tanh_out_141 = arith.divf %cst_3, %tanh_out_140 : tensor<128x64xf32, #mma> loc(#loc564)
        %tanh_out_142 = arith.mulf %tanh_out_141, %cst_0 : tensor<128x64xf32, #mma> loc(#loc558)
        %tanh_out_143 = arith.subf %tanh_out_142, %cst_3 : tensor<128x64xf32, #mma> loc(#loc559)
        %p_144 = arith.mulf %qk_131, %cst_2 : tensor<128x64xf32, #mma> loc(#loc536)
        %p_145 = arith.mulf %tanh_out_143, %tanh_out_143 : tensor<128x64xf32, #mma> loc(#loc537)
        %p_146 = arith.subf %cst_3, %p_145 : tensor<128x64xf32, #mma> loc(#loc538)
        %p_147 = arith.mulf %qk_131, %cst_1 : tensor<128x64xf32, #mma> loc(#loc539)
        %p_148 = arith.mulf %p_147, %qk_131 : tensor<128x64xf32, #mma> loc(#loc540)
        %p_149 = arith.addf %p_148, %cst_5 : tensor<128x64xf32, #mma> loc(#loc541)
        %p_150 = arith.mulf %p_146, %p_149 : tensor<128x64xf32, #mma> loc(#loc542)
        %p_151 = arith.mulf %p_144, %p_150 : tensor<128x64xf32, #mma> loc(#loc543)
        %p_152 = arith.addf %tanh_out_143, %cst_3 : tensor<128x64xf32, #mma> loc(#loc544)
        %p_153 = arith.mulf %p_152, %cst_2 : tensor<128x64xf32, #mma> loc(#loc545)
        %p_154 = arith.addf %p_151, %p_153 : tensor<128x64xf32, #mma> loc(#loc546)
        %p_155 = arith.mulf %p_154, %p : tensor<128x64xf32, #mma> loc(#loc507)
        %dp_156 = tt.dot %do_53, %vT_130, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc521)
        %ds_157 = arith.mulf %p_155, %dp_156 : tensor<128x64xf32, #mma> loc(#loc522)
        %ds_158 = arith.truncf %ds_157 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc523)
        %ds_159 = ttg.convert_layout %ds_158 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc523)
        %dq_160 = tt.dot %ds_159, %dq_127, %arg22 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc517)
        %curr_n_161 = arith.addi %arg26, %c1_i32 : i32 loc(#loc555)
        %curr_n_162 = arith.cmpi slt, %curr_n_161, %c1_i32 : i32 loc(#loc555)
        %curr_n_163 = arith.select %curr_n_162, %curr_n_161, %c0_i32 : i32 loc(#loc555)
        %vT_164 = ttg.memdesc_index %vT[%curr_n_163] : !ttg.memdesc<1x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> loc(#loc509)
        ttg.local_store %vT_129, %vT_164 : tensor<64x64xbf16, #blocked> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> loc(#loc509)
        scf.yield %dq_160, %curr_n_116, %kT_ptrs_117, %vT_ptrs_118, %curr_n_163, %kT_124, %vT_164 : tensor<128x64xf32, #mma>, i32, !tt.ptr<bf16>, !tt.ptr<bf16>, i32, tensor<64x64xbf16, #blocked>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> loc(#loc555)
      } {tt.loop_unroll_factor = 1 : i32} loc(#loc555)
      %curr_n_74 = arith.cmpi sge, %num_steps_54, %c1_i32 : i32 loc(#loc555)
      %kT_75 = ttg.local_alloc %curr_n_73#5 : (tensor<64x64xbf16, #blocked>) -> !ttg.memdesc<64x64xbf16, #shared2, #smem> loc(#loc511)
      %dq = ttg.local_load %kT_75 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #linear1> loc(#loc517)
      %dq_76 = tt.trans %dq {order = array<i32: 1, 0>} : tensor<64x64xbf16, #linear1> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc518)
      %kT_77 = ttg.local_load %kT_75 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc511)
      %vT_78 = ttg.local_load %curr_n_73#6 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 1x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc509)
      %qk = scf.if %curr_n_74 -> (tensor<128x64xf32, #mma>) {
        %qk_112 = tt.dot %q_45, %kT_77, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc519)
        scf.yield %qk_112 : tensor<128x64xf32, #mma> loc(#loc519)
      } else {
        scf.yield %cst : tensor<128x64xf32, #mma> loc(#loc519)
      } loc(#loc519)
      %tanh_out = arith.mulf %qk, %cst_5 : tensor<128x64xf32, #mma> loc(#loc530)
      %tanh_out_79 = arith.mulf %qk, %cst_4 : tensor<128x64xf32, #mma> loc(#loc531)
      %tanh_out_80 = arith.mulf %tanh_out_79, %qk : tensor<128x64xf32, #mma> loc(#loc532)
      %tanh_out_81 = arith.addf %tanh_out_80, %cst_3 : tensor<128x64xf32, #mma> loc(#loc533)
      %tanh_out_82 = arith.mulf %tanh_out, %tanh_out_81 : tensor<128x64xf32, #mma> loc(#loc534)
      %tanh_out_83 = arith.mulf %tanh_out_82, %cst_0 : tensor<128x64xf32, #mma> loc(#loc556)
      %tanh_out_84 = arith.subf %cst, %tanh_out_83 : tensor<128x64xf32, #mma> loc(#loc561)
      %tanh_out_85 = math.exp %tanh_out_84 : tensor<128x64xf32, #mma> loc(#loc562)
      %tanh_out_86 = arith.addf %tanh_out_85, %cst_3 : tensor<128x64xf32, #mma> loc(#loc563)
      %tanh_out_87 = arith.divf %cst_3, %tanh_out_86 : tensor<128x64xf32, #mma> loc(#loc564)
      %tanh_out_88 = arith.mulf %tanh_out_87, %cst_0 : tensor<128x64xf32, #mma> loc(#loc558)
      %tanh_out_89 = arith.subf %tanh_out_88, %cst_3 : tensor<128x64xf32, #mma> loc(#loc559)
      %p_90 = arith.mulf %qk, %cst_2 : tensor<128x64xf32, #mma> loc(#loc536)
      %p_91 = arith.mulf %tanh_out_89, %tanh_out_89 : tensor<128x64xf32, #mma> loc(#loc537)
      %p_92 = arith.subf %cst_3, %p_91 : tensor<128x64xf32, #mma> loc(#loc538)
      %p_93 = arith.mulf %qk, %cst_1 : tensor<128x64xf32, #mma> loc(#loc539)
      %p_94 = arith.mulf %p_93, %qk : tensor<128x64xf32, #mma> loc(#loc540)
      %p_95 = arith.addf %p_94, %cst_5 : tensor<128x64xf32, #mma> loc(#loc541)
      %p_96 = arith.mulf %p_92, %p_95 : tensor<128x64xf32, #mma> loc(#loc542)
      %p_97 = arith.mulf %p_90, %p_96 : tensor<128x64xf32, #mma> loc(#loc543)
      %p_98 = arith.addf %tanh_out_89, %cst_3 : tensor<128x64xf32, #mma> loc(#loc544)
      %p_99 = arith.mulf %p_98, %cst_2 : tensor<128x64xf32, #mma> loc(#loc545)
      %p_100 = arith.addf %p_97, %p_99 : tensor<128x64xf32, #mma> loc(#loc546)
      %p_101 = arith.mulf %p_100, %p : tensor<128x64xf32, #mma> loc(#loc507)
      %dp = scf.if %curr_n_74 -> (tensor<128x64xf32, #mma>) {
        %dp_112 = tt.dot %do_53, %vT_78, %cst : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc521)
        scf.yield %dp_112 : tensor<128x64xf32, #mma> loc(#loc521)
      } else {
        scf.yield %cst : tensor<128x64xf32, #mma> loc(#loc521)
      } loc(#loc521)
      %ds = arith.mulf %p_101, %dp : tensor<128x64xf32, #mma> loc(#loc522)
      %ds_102 = arith.truncf %ds : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc523)
      %ds_103 = ttg.convert_layout %ds_102 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc523)
      %dq_104 = scf.if %curr_n_74 -> (tensor<128x64xf32, #mma>) {
        %dq_112 = tt.dot %ds_103, %dq_76, %curr_n_73#0 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc517)
        scf.yield %dq_112 : tensor<128x64xf32, #mma> loc(#loc517)
      } else {
        scf.yield %curr_n_73#0 : tensor<128x64xf32, #mma> loc(#loc517)
      } loc(#loc517)
      %curr_n_105 = arith.select %curr_n_74, %dq_104, %curr_n_73#0 : tensor<128x64xf32, #mma> loc(#loc555)
      ttg.local_dealloc %vT : !ttg.memdesc<1x64x64xbf16, #shared2, #smem, mutable> loc(#loc555)
      %dq_ptrs = tt.expand_dims %offs_m {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc486)
      %dq_ptrs_106 = tt.splat %stride_qm : i32 -> tensor<128x1xi32, #linear> loc(#loc486)
      %dq_ptrs_107 = arith.muli %dq_ptrs, %dq_ptrs_106 : tensor<128x1xi32, #linear> loc(#loc486)
      %dq_ptrs_108 = tt.addptr %3#4, %q_36 : !tt.ptr<bf16>, i32 loc(#loc486)
      %dq_ptrs_109 = tt.broadcast %dq_ptrs_107 : tensor<128x1xi32, #linear> -> tensor<128x64xi32, #linear> loc(#loc487)
      %dq_ptrs_110 = tt.broadcast %qmask_20 : tensor<1x64xi32, #linear> -> tensor<128x64xi32, #linear> loc(#loc487)
      %dq_ptrs_111 = arith.addi %dq_ptrs_110, %dq_ptrs_109 : tensor<128x64xi32, #linear> loc(#loc487)
      %6 = arith.truncf %curr_n_105 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc372)
      %7 = ttg.convert_layout %6 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #linear> loc(#loc372)
      amdgpu.buffer_store %7, %dq_ptrs_108[%dq_ptrs_111], %qmask_34 stride = %stride_qm : tensor<128x64xbf16, #linear> loc(#loc372)
    } loc(#loc325)
    tt.return loc(#loc191)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1641:8)
#loc3 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1564:14)
#loc4 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1565:14)
#loc5 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1566:14)
#loc6 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1567:14)
#loc7 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1568:14)
#loc8 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1569:14)
#loc9 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1570:14)
#loc10 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1575:30)
#loc11 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1583:30)
#loc12 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1584:24)
#loc13 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1586:23)
#loc14 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1158:34)
#loc15 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1158:22)
#loc16 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1159:42)
#loc17 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1159:20)
#loc18 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1161:19)
#loc19 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1173:38)
#loc20 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1173:26)
#loc21 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1174:44)
#loc22 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1174:24)
#loc23 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1176:23)
#loc24 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1182:20)
#loc25 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1186:26)
#loc26 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1287:18)
#loc27 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1287:37)
#loc28 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1287:26)
#loc29 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1287:7)
#loc30 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1288:40)
#loc31 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1288:26)
#loc32 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1290:26)
#loc33 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1291:24)
#loc34 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1291:46)
#loc35 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1291:36)
#loc36 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1292:26)
#loc37 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1292:48)
#loc38 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1292:38)
#loc39 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1293:25)
#loc40 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1293:48)
#loc41 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1293:38)
#loc42 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1296:13)
#loc43 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1297:13)
#loc44 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1298:13)
#loc45 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1299:14)
#loc46 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1300:14)
#loc47 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1301:14)
#loc48 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1302:14)
#loc49 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1305:17)
#loc50 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1305:7)
#loc51 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1306:40)
#loc52 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1306:27)
#loc53 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1307:24)
#loc54 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1307:35)
#loc55 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1307:55)
#loc56 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1307:66)
#loc57 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1307:48)
#loc58 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1330:20)
#loc59 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1330:50)
#loc60 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1330:16)
#loc61 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1334:20)
#loc62 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1334:16)
#loc63 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":41:22)
#loc64 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1339:52)
#loc65 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":41:28)
#loc66 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":778:22)
#loc67 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1380:12)
#loc68 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":778:59)
#loc69 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":778:52)
#loc70 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":779:23)
#loc71 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":779:54)
#loc72 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":789:35)
#loc73 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":789:66)
#loc74 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":789:48)
#loc75 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":808:35)
#loc76 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":808:44)
#loc77 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":841:15)
#loc78 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":893:32)
#loc79 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":894:32)
#loc80 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":787:46)
#loc81 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":802:25)
#loc82 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":818:25)
#loc83 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":891:18)
#loc84 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":893:23)
#loc85 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":894:23)
#loc86 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":788:26)
#loc87 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":789:55)
#loc88 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":872:26)
#loc89 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":872:35)
#loc90 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":805:24)
#loc91 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":808:24)
#loc92 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":820:24)
#loc93 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":820:33)
#loc94 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:55)
#loc95 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:76)
#loc96 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:81)
#loc97 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:65)
#loc98 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:61)
#loc99 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":67:30)
#loc100 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":86:16)
#loc101 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":836:40)
#loc102 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":48:30)
#loc103 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":67:26)
#loc104 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":48:29)
#loc105 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":48:20)
#loc106 = loc("/workspace/projects/triton-openai/python/triton/language/standard.py":48:16)
#loc107 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":67:15)
#loc108 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":67:35)
#loc109 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":837:24)
#loc110 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":837:34)
#loc111 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":837:30)
#loc112 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":842:21)
#loc113 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":843:26)
#loc114 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":855:34)
#loc115 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":855:23)
#loc116 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":855:19)
#loc117 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":856:49)
#loc118 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":856:54)
#loc119 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":856:34)
#loc120 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":856:19)
#loc121 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":857:23)
#loc122 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":857:16)
#loc123 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":860:14)
#loc124 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":870:19)
#loc125 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":871:21)
#loc126 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1391:23)
#loc127 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1391:53)
#loc128 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1404:34)
#loc129 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1407:23)
#loc130 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1420:34)
#loc131 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1425:21)
#loc132 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1425:11)
#loc133 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1427:44)
#loc134 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1427:31)
#loc135 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1429:28)
#loc136 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1429:39)
#loc137 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1429:59)
#loc138 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1429:70)
#loc139 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1429:52)
#loc140 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1446:24)
#loc141 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1446:54)
#loc142 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1446:20)
#loc143 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1450:25)
#loc144 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1450:56)
#loc145 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1450:20)
#loc146 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1456:39)
#loc147 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":944:22)
#loc148 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1492:16)
#loc149 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":944:59)
#loc150 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":944:52)
#loc151 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":953:35)
#loc152 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":953:66)
#loc153 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":953:48)
#loc154 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":985:13)
#loc155 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1006:32)
#loc156 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":973:25)
#loc157 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":951:46)
#loc158 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":972:25)
#loc159 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1004:18)
#loc160 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1006:23)
#loc161 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1007:23)
#loc162 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":952:26)
#loc163 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":953:55)
#loc164 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1002:25)
#loc165 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1002:34)
#loc166 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":975:23)
#loc167 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:47)
#loc168 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":982:31)
#loc169 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:69)
#loc170 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:73)
#loc171 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:58)
#loc172 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:52)
#loc173 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":112:32)
#loc174 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":113:17)
#loc175 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:24)
#loc176 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:13)
#loc177 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:67)
#loc178 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:71)
#loc179 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:52)
#loc180 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:37)
#loc181 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":114:8)
#loc182 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":115:19)
#loc183 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":115:15)
#loc184 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/math.py":115:8)
#loc185 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":997:24)
#loc186 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":998:17)
#loc187 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":999:19)
#loc188 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1501:27)
#loc189 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1501:57)
#loc190 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1516:38)
#loc191 = loc("/workspace/projects/tritonbench/tritonbench/operators/gdpa/gdpa.py":1594:4)
#loc213 = loc(callsite(#loc1 at #loc2))
#loc214 = loc("off_z"(#loc10))
#loc215 = loc("off_seq_h"(#loc11))
#loc216 = loc("off_h"(#loc12))
#loc217 = loc("pid"(#loc13))
#loc218 = loc("begin_q"(#loc14))
#loc219 = loc("begin_q"(#loc15))
#loc220 = loc("end_q"(#loc16))
#loc221 = loc("end_q"(#loc17))
#loc222 = loc("qlen"(#loc18))
#loc223 = loc("begin_k"(#loc19))
#loc224 = loc("begin_k"(#loc20))
#loc225 = loc("end_k"(#loc21))
#loc226 = loc("end_k"(#loc22))
#loc227 = loc("klen"(#loc23))
#loc228 = loc("start_n"(#loc24))
#loc229 = loc("offs_k"(#loc25))
#loc230 = loc(callsite(#loc26 at #loc2))
#loc231 = loc(callsite(#loc27 at #loc2))
#loc232 = loc(callsite(#loc28 at #loc2))
#loc233 = loc(callsite(#loc29 at #loc2))
#loc234 = loc("begin_o"(#loc30))
#loc235 = loc("begin_o"(#loc31))
#loc236 = loc("off_h2"(#loc32))
#loc237 = loc("qadj"(#loc33))
#loc238 = loc("qadj"(#loc34))
#loc239 = loc("qadj"(#loc35))
#loc240 = loc("kadj"(#loc36))
#loc241 = loc("kadj"(#loc37))
#loc242 = loc("kadj"(#loc38))
#loc243 = loc("doadj"(#loc39))
#loc244 = loc("doadj"(#loc40))
#loc245 = loc("doadj"(#loc41))
#loc246 = loc("Q"(#loc42))
#loc247 = loc("K"(#loc43))
#loc248 = loc("V"(#loc44))
#loc249 = loc("DO"(#loc45))
#loc250 = loc("DQ"(#loc46))
#loc251 = loc("DK"(#loc47))
#loc252 = loc("DV"(#loc48))
#loc253 = loc(callsite(#loc49 at #loc2))
#loc254 = loc(callsite(#loc50 at #loc2))
#loc255 = loc("offs_n"(#loc51))
#loc256 = loc("offs_n"(#loc52))
#loc257 = loc("kmask"(#loc53))
#loc258 = loc("kmask"(#loc54))
#loc259 = loc("kmask"(#loc55))
#loc260 = loc("kmask"(#loc56))
#loc261 = loc("kmask"(#loc57))
#loc262 = loc("k"(#loc58))
#loc263 = loc("k"(#loc59))
#loc264 = loc("k"(#loc60))
#loc265 = loc("v"(#loc61))
#loc266 = loc("v"(#loc62))
#loc267 = loc("num_steps"(#loc64))
#loc268 = loc("qT_ptrs"(#loc66))
#loc269 = loc(callsite(#loc67 at #loc2))
#loc270 = loc("qT_ptrs"(#loc68))
#loc271 = loc("qT_ptrs"(#loc69))
#loc272 = loc("do_ptrs"(#loc70))
#loc273 = loc("do_ptrs"(#loc71))
#loc274 = loc("qmask"(#loc72))
#loc275 = loc("qmask"(#loc73))
#loc276 = loc("qmask"(#loc74))
#loc277 = loc("omask"(#loc75))
#loc278 = loc("omask"(#loc76))
#loc279 = loc("ppT"(#loc77))
#loc280 = loc("qT_ptrs"(#loc78))
#loc281 = loc("do_ptrs"(#loc79))
#loc282 = loc("dk"(#loc80))
#loc283 = loc("qT"(#loc81))
#loc284 = loc("do"(#loc82))
#loc285 = loc("curr_m"(#loc83))
#loc286 = loc("qT_ptrs"(#loc84))
#loc287 = loc("do_ptrs"(#loc85))
#loc288 = loc("offs_m"(#loc86))
#loc289 = loc("qmask"(#loc87))
#loc290 = loc("dk"(#loc88))
#loc291 = loc("dk"(#loc89))
#loc292 = loc("qkT"(#loc90))
#loc293 = loc("omask"(#loc91))
#loc294 = loc("dpT"(#loc92))
#loc295 = loc("dpT"(#loc93))
#loc296 = loc("tanh_out"(#loc94))
#loc297 = loc("tanh_out"(#loc95))
#loc298 = loc("tanh_out"(#loc96))
#loc299 = loc("tanh_out"(#loc97))
#loc300 = loc("tanh_out"(#loc98))
#loc301 = loc("tanh_out"(#loc101))
#loc302 = loc("ppT"(#loc109))
#loc303 = loc("ppT"(#loc110))
#loc304 = loc("ppT"(#loc111))
#loc305 = loc("ppT"(#loc112))
#loc306 = loc("dv"(#loc113))
#loc307 = loc("pT"(#loc114))
#loc308 = loc("pT"(#loc115))
#loc309 = loc("pT"(#loc116))
#loc310 = loc("pT"(#loc117))
#loc311 = loc("pT"(#loc118))
#loc312 = loc("pT"(#loc119))
#loc313 = loc("pT"(#loc120))
#loc314 = loc("pT"(#loc121))
#loc315 = loc("pT"(#loc122))
#loc316 = loc("pT"(#loc123))
#loc317 = loc("dsT"(#loc124))
#loc318 = loc("dsT"(#loc125))
#loc319 = loc("dv_ptrs"(#loc126))
#loc320 = loc("dv_ptrs"(#loc127))
#loc321 = loc(callsite(#loc128 at #loc2))
#loc322 = loc("dk_ptrs"(#loc129))
#loc323 = loc(callsite(#loc130 at #loc2))
#loc324 = loc(callsite(#loc131 at #loc2))
#loc325 = loc(callsite(#loc132 at #loc2))
#loc326 = loc("offs_m"(#loc133))
#loc327 = loc("offs_m"(#loc134))
#loc328 = loc("qmask"(#loc135))
#loc329 = loc("qmask"(#loc136))
#loc330 = loc("qmask"(#loc137))
#loc331 = loc("qmask"(#loc138))
#loc332 = loc("qmask"(#loc139))
#loc333 = loc("q"(#loc140))
#loc334 = loc("q"(#loc141))
#loc335 = loc("q"(#loc142))
#loc336 = loc("do"(#loc143))
#loc337 = loc("do"(#loc144))
#loc338 = loc("do"(#loc145))
#loc339 = loc("num_steps"(#loc146))
#loc340 = loc("kT_ptrs"(#loc147))
#loc341 = loc("dq"(#loc148))
#loc342 = loc("kT_ptrs"(#loc149))
#loc343 = loc("kT_ptrs"(#loc150))
#loc344 = loc("kmask"(#loc151))
#loc345 = loc("kmask"(#loc152))
#loc346 = loc("kmask"(#loc153))
#loc347 = loc("p"(#loc154))
#loc348 = loc("kT_ptrs"(#loc155))
#loc349 = loc("vT"(#loc156))
#loc350 = loc("dq"(#loc157))
#loc351 = loc("kT"(#loc158))
#loc352 = loc("curr_n"(#loc159))
#loc353 = loc("kT_ptrs"(#loc160))
#loc354 = loc("vT_ptrs"(#loc161))
#loc355 = loc("offs_n"(#loc162))
#loc356 = loc("kmask"(#loc163))
#loc357 = loc("dq"(#loc164))
#loc358 = loc("dq"(#loc165))
#loc359 = loc("qk"(#loc166))
#loc360 = loc("tanh_out"(#loc167))
#loc361 = loc("p"(#loc168))
#loc362 = loc("tanh_out"(#loc169))
#loc363 = loc("tanh_out"(#loc170))
#loc364 = loc("tanh_out"(#loc171))
#loc365 = loc("tanh_out"(#loc172))
#loc366 = loc("tanh_out"(#loc173))
#loc367 = loc("dp"(#loc185))
#loc368 = loc("ds"(#loc186))
#loc369 = loc("ds"(#loc187))
#loc370 = loc("dq_ptrs"(#loc188))
#loc371 = loc("dq_ptrs"(#loc189))
#loc372 = loc(callsite(#loc190 at #loc2))
#loc373 = loc(callsite(#loc218 at #loc2))
#loc374 = loc(callsite(#loc219 at #loc2))
#loc375 = loc(callsite(#loc220 at #loc2))
#loc376 = loc(callsite(#loc221 at #loc2))
#loc377 = loc(callsite(#loc222 at #loc2))
#loc378 = loc(callsite(#loc223 at #loc2))
#loc379 = loc(callsite(#loc224 at #loc2))
#loc380 = loc(callsite(#loc225 at #loc2))
#loc381 = loc(callsite(#loc226 at #loc2))
#loc382 = loc(callsite(#loc227 at #loc2))
#loc383 = loc(callsite(#loc228 at #loc2))
#loc384 = loc(callsite(#loc229 at #loc2))
#loc385 = loc(callsite(#loc234 at #loc2))
#loc386 = loc(callsite(#loc235 at #loc2))
#loc387 = loc(callsite(#loc236 at #loc2))
#loc388 = loc(callsite(#loc237 at #loc2))
#loc389 = loc(callsite(#loc238 at #loc2))
#loc390 = loc(callsite(#loc239 at #loc2))
#loc391 = loc(callsite(#loc240 at #loc2))
#loc392 = loc(callsite(#loc241 at #loc2))
#loc393 = loc(callsite(#loc242 at #loc2))
#loc394 = loc(callsite(#loc243 at #loc2))
#loc395 = loc(callsite(#loc244 at #loc2))
#loc396 = loc(callsite(#loc245 at #loc2))
#loc397 = loc("Q"(#loc246))
#loc398 = loc("K"(#loc247))
#loc399 = loc("V"(#loc248))
#loc400 = loc("DO"(#loc249))
#loc401 = loc("DQ"(#loc250))
#loc402 = loc("DK"(#loc251))
#loc403 = loc("DV"(#loc252))
#loc404 = loc(callsite(#loc255 at #loc2))
#loc405 = loc(callsite(#loc256 at #loc2))
#loc406 = loc(callsite(#loc257 at #loc2))
#loc407 = loc(callsite(#loc258 at #loc2))
#loc408 = loc(callsite(#loc259 at #loc2))
#loc409 = loc(callsite(#loc260 at #loc2))
#loc410 = loc(callsite(#loc261 at #loc2))
#loc411 = loc(callsite(#loc262 at #loc2))
#loc412 = loc(callsite(#loc263 at #loc2))
#loc413 = loc(callsite(#loc264 at #loc2))
#loc414 = loc(callsite(#loc265 at #loc2))
#loc415 = loc(callsite(#loc266 at #loc2))
#loc416 = loc(callsite(#loc267 at #loc2))
#loc417 = loc(callsite(#loc268 at #loc269))
#loc418 = loc(callsite(#loc270 at #loc269))
#loc419 = loc(callsite(#loc271 at #loc269))
#loc420 = loc(callsite(#loc272 at #loc269))
#loc421 = loc(callsite(#loc273 at #loc269))
#loc422 = loc(callsite(#loc274 at #loc269))
#loc423 = loc(callsite(#loc275 at #loc269))
#loc424 = loc(callsite(#loc276 at #loc269))
#loc425 = loc(callsite(#loc277 at #loc269))
#loc426 = loc(callsite(#loc278 at #loc269))
#loc427 = loc(callsite(#loc279 at #loc269))
#loc428 = loc(callsite(#loc280 at #loc269))
#loc429 = loc(callsite(#loc281 at #loc269))
#loc430 = loc("dv"(#loc282))
#loc431 = loc(callsite(#loc283 at #loc269))
#loc432 = loc(callsite(#loc284 at #loc269))
#loc433 = loc(callsite(#loc285 at #loc269))
#loc434 = loc(callsite(#loc286 at #loc269))
#loc435 = loc(callsite(#loc287 at #loc269))
#loc436 = loc(callsite(#loc288 at #loc269))
#loc437 = loc(callsite(#loc289 at #loc269))
#loc438 = loc(callsite(#loc290 at #loc269))
#loc439 = loc(callsite(#loc291 at #loc269))
#loc440 = loc(callsite(#loc292 at #loc269))
#loc441 = loc(callsite(#loc293 at #loc269))
#loc442 = loc(callsite(#loc294 at #loc269))
#loc443 = loc(callsite(#loc295 at #loc269))
#loc444 = loc(callsite(#loc296 at #loc269))
#loc445 = loc(callsite(#loc297 at #loc269))
#loc446 = loc(callsite(#loc298 at #loc269))
#loc447 = loc(callsite(#loc299 at #loc269))
#loc448 = loc(callsite(#loc300 at #loc269))
#loc449 = loc(callsite(#loc301 at #loc269))
#loc450 = loc(callsite(#loc302 at #loc269))
#loc451 = loc(callsite(#loc303 at #loc269))
#loc452 = loc(callsite(#loc304 at #loc269))
#loc453 = loc(callsite(#loc305 at #loc269))
#loc454 = loc(callsite(#loc306 at #loc269))
#loc455 = loc(callsite(#loc307 at #loc269))
#loc456 = loc(callsite(#loc308 at #loc269))
#loc457 = loc(callsite(#loc309 at #loc269))
#loc458 = loc(callsite(#loc310 at #loc269))
#loc459 = loc(callsite(#loc311 at #loc269))
#loc460 = loc(callsite(#loc312 at #loc269))
#loc461 = loc(callsite(#loc313 at #loc269))
#loc462 = loc(callsite(#loc314 at #loc269))
#loc463 = loc(callsite(#loc315 at #loc269))
#loc464 = loc(callsite(#loc316 at #loc269))
#loc465 = loc(callsite(#loc317 at #loc269))
#loc466 = loc(callsite(#loc318 at #loc269))
#loc467 = loc(callsite(#loc319 at #loc2))
#loc468 = loc(callsite(#loc320 at #loc2))
#loc469 = loc(callsite(#loc322 at #loc2))
#loc470 = loc(callsite(#loc326 at #loc2))
#loc471 = loc(callsite(#loc327 at #loc2))
#loc472 = loc(callsite(#loc328 at #loc2))
#loc473 = loc(callsite(#loc329 at #loc2))
#loc474 = loc(callsite(#loc330 at #loc2))
#loc475 = loc(callsite(#loc331 at #loc2))
#loc476 = loc(callsite(#loc332 at #loc2))
#loc477 = loc(callsite(#loc333 at #loc2))
#loc478 = loc(callsite(#loc334 at #loc2))
#loc479 = loc(callsite(#loc335 at #loc2))
#loc480 = loc(callsite(#loc336 at #loc2))
#loc481 = loc(callsite(#loc337 at #loc2))
#loc482 = loc(callsite(#loc338 at #loc2))
#loc483 = loc(callsite(#loc339 at #loc2))
#loc484 = loc(callsite(#loc341 at #loc2))
#loc485 = loc("offs_n"(#loc350))
#loc486 = loc(callsite(#loc370 at #loc2))
#loc487 = loc(callsite(#loc371 at #loc2))
#loc488 = loc(callsite(#loc397 at #loc2))
#loc489 = loc(callsite(#loc398 at #loc2))
#loc490 = loc(callsite(#loc399 at #loc2))
#loc491 = loc(callsite(#loc400 at #loc2))
#loc492 = loc(callsite(#loc401 at #loc2))
#loc493 = loc(callsite(#loc402 at #loc2))
#loc494 = loc(callsite(#loc403 at #loc2))
#loc495 = loc(callsite(#loc63 at #loc416))
#loc496 = loc(callsite(#loc65 at #loc416))
#loc497 = loc("offs_m"(#loc430))
#loc498 = loc(callsite(#loc100 at #loc449))
#loc499 = loc(callsite(#loc63 at #loc483))
#loc500 = loc(callsite(#loc65 at #loc483))
#loc501 = loc(callsite(#loc340 at #loc484))
#loc502 = loc(callsite(#loc342 at #loc484))
#loc503 = loc(callsite(#loc343 at #loc484))
#loc504 = loc(callsite(#loc344 at #loc484))
#loc505 = loc(callsite(#loc345 at #loc484))
#loc506 = loc(callsite(#loc346 at #loc484))
#loc507 = loc(callsite(#loc347 at #loc484))
#loc508 = loc(callsite(#loc348 at #loc484))
#loc509 = loc(callsite(#loc349 at #loc484))
#loc510 = loc("kT_ptrs"(#loc485))
#loc511 = loc(callsite(#loc351 at #loc484))
#loc512 = loc(callsite(#loc352 at #loc484))
#loc513 = loc(callsite(#loc353 at #loc484))
#loc514 = loc(callsite(#loc354 at #loc484))
#loc515 = loc(callsite(#loc355 at #loc484))
#loc516 = loc(callsite(#loc356 at #loc484))
#loc517 = loc(callsite(#loc357 at #loc484))
#loc518 = loc(callsite(#loc358 at #loc484))
#loc519 = loc(callsite(#loc359 at #loc484))
#loc520 = loc(callsite(#loc361 at #loc484))
#loc521 = loc(callsite(#loc367 at #loc484))
#loc522 = loc(callsite(#loc368 at #loc484))
#loc523 = loc(callsite(#loc369 at #loc484))
#loc524 = loc("qT_ptrs"(#loc497))
#loc525 = loc(callsite(#loc99 at #loc498))
#loc526 = loc(callsite(#loc103 at #loc498))
#loc527 = loc(callsite(#loc107 at #loc498))
#loc528 = loc(callsite(#loc108 at #loc498))
#loc529 = loc("vT_ptrs"(#loc510))
#loc530 = loc(callsite(#loc360 at #loc520))
#loc531 = loc(callsite(#loc362 at #loc520))
#loc532 = loc(callsite(#loc363 at #loc520))
#loc533 = loc(callsite(#loc364 at #loc520))
#loc534 = loc(callsite(#loc365 at #loc520))
#loc535 = loc(callsite(#loc366 at #loc520))
#loc536 = loc(callsite(#loc174 at #loc520))
#loc537 = loc(callsite(#loc175 at #loc520))
#loc538 = loc(callsite(#loc176 at #loc520))
#loc539 = loc(callsite(#loc177 at #loc520))
#loc540 = loc(callsite(#loc178 at #loc520))
#loc541 = loc(callsite(#loc179 at #loc520))
#loc542 = loc(callsite(#loc180 at #loc520))
#loc543 = loc(callsite(#loc181 at #loc520))
#loc544 = loc(callsite(#loc182 at #loc520))
#loc545 = loc(callsite(#loc183 at #loc520))
#loc546 = loc(callsite(#loc184 at #loc520))
#loc547 = loc("do_ptrs"(#loc524))
#loc548 = loc(callsite(#loc102 at #loc526))
#loc549 = loc(callsite(#loc104 at #loc526))
#loc550 = loc(callsite(#loc105 at #loc526))
#loc551 = loc(callsite(#loc106 at #loc526))
#loc552 = loc("curr_n"(#loc529))
#loc553 = loc(callsite(#loc100 at #loc535))
#loc554 = loc("curr_m"(#loc547))
#loc555 = loc(callsite(#loc552 at #loc484))
#loc556 = loc(callsite(#loc99 at #loc553))
#loc557 = loc(callsite(#loc103 at #loc553))
#loc558 = loc(callsite(#loc107 at #loc553))
#loc559 = loc(callsite(#loc108 at #loc553))
#loc560 = loc(callsite(#loc554 at #loc269))
#loc561 = loc(callsite(#loc102 at #loc557))
#loc562 = loc(callsite(#loc104 at #loc557))
#loc563 = loc(callsite(#loc105 at #loc557))
#loc564 = loc(callsite(#loc106 at #loc557))
