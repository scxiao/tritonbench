# =================================================================
# This file is generated by benchmarks/gen_metadata/run.py
# List of operators that have baseline benchmark for accuracy and speedup.
# =================================================================
addmm: aten_addmm
bf16xint16_gemm: bf16xbf16
cross_entropy: cross_entropy_loss
embedding: torch_embedding
flash_attention: flash_v3
flex_attention: eager
fp8_gemm: torch_fp8_gemm
fp8_gemm_blockwise: _cutlass
fp8_gemm_rowwise: _cutlass_or_ck
fp8_gemm_rowwise_grouped: _cutlass_or_ck
fused_linear_cross_entropy: torch_lm_head_ce
fused_linear_jsd: torch_lm_head_jsd
gather_gemv: test_eager
geglu: torch_geglu
gemm: aten_matmul
grouped_gemm: torch
int4_gemm: tinygemm
jagged_layer_norm: torch_jagged_layer_norm_unbind_torch_layer_norm
jagged_mean: torch_jagged_mean_unbind_torch_mean
jagged_softmax: torch_jagged_softmax_unbind_torch_softmax
jagged_sum: torch_jagged_sum_no_pad
jsd: torch_jsd
kl_div: torch_kl_div
launch_latency: nop_python_function
layer_norm: torch_layer_norm
low_mem_dropout: torch_dropout
mixed_gemm: aten_bf16_bf16
rms_norm: llama_rms
rope: apply_rotary_pos_emb
softmax: naive_softmax
sum: torch_sum
swiglu: torch_swiglu
template_attention: test_no_exp2
vector_add: torch_add
welford: test_no_welford
